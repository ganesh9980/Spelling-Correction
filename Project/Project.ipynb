{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENTER THE TEXT : The purpose of life is to live it, to taste experience to the utmost, to reach out eagerly and without fear for newer and richer experience.\n",
      "-\n",
      "Input sentence:   yThe prpose of liife is to ilve it to taste experience to zthe utmots to reach oOut eagelry Dnd without faer for enwer Dnd richek experience\n",
      "-\n",
      "Decoded sentence: The purpose of life is to live it to taste experience to the utmost to reach Out eagerly Dend without fare for ender Dend riches experience\n",
      "-\n",
      "Target sentence:  The purpose of life is to live it to taste experience to the utmost to reach out eagerly and without fear for newer and richer experience\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import unidecode\n",
    "import numpy as np\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input\n",
    "np.random.seed(1234)\n",
    "SOS = '\\t' # start of sequence.\n",
    "EOS = '*' # end of sequence.\n",
    "CHARS = list('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ ')\n",
    "REMOVE_CHARS = '[#$%\"\\+@<=>!&,-.?:;()*\\[\\]^_`{|}~/\\d\\t\\n\\r\\x0b\\x0c]'\n",
    "\n",
    "\n",
    "class CharacterTable(object):\n",
    "    def __init__(self, chars):\n",
    "        self.chars = sorted(set(chars))\n",
    "        self.char2index = dict((c, i) for i, c in enumerate(self.chars))\n",
    "        self.index2char = dict((i, c) for i, c in enumerate(self.chars))\n",
    "        self.size = len(self.chars)\n",
    "    \n",
    "    def encode(self, C, nb_rows):\n",
    "        x = np.zeros((nb_rows, len(self.chars)), dtype=np.float32)\n",
    "        for i, c in enumerate(C):\n",
    "            x[i, self.char2index[c]] = 1.0\n",
    "        return x\n",
    "\n",
    "    def decode(self, x, calc_argmax=True):\n",
    "        if calc_argmax:\n",
    "            indices = x.argmax(axis=-1)\n",
    "        else:\n",
    "            indices = x\n",
    "        chars = ''.join(self.index2char[ind] for ind in indices)\n",
    "        return indices, chars\n",
    "\n",
    "    def sample_multinomial(self, preds, temperature=1.0):\n",
    "        # Reshaped to 1D array of shape (nb_chars,).\n",
    "        preds = np.reshape(preds, len(self.chars)).astype(np.float64)\n",
    "        preds = np.log(preds) / temperature\n",
    "        exp_preds = np.exp(preds)\n",
    "        preds = exp_preds / np.sum(exp_preds)\n",
    "        probs = np.random.multinomial(1,preds,1)\n",
    "        index = np.argmax(probs)\n",
    "        char  = self.index2char[index]\n",
    "        return index, char\n",
    "\n",
    "\n",
    "def read_text(data_path, list_of_books):\n",
    "    text = ''\n",
    "    for book in list_of_books:\n",
    "        file_path = os.path.join(data_path, book)\n",
    "        strings = unidecode.unidecode(open(file_path).read())\n",
    "        text += strings + ' '\n",
    "    return text\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = [re.sub(REMOVE_CHARS, '', token)\n",
    "              for token in re.split(\"[-\\n ]\", text)]\n",
    "    return tokens\n",
    "\n",
    "    \n",
    "def add_speling_erors(token, error_rate):\n",
    "    \"\"\"Simulate some artificial spelling mistakes.\"\"\"\n",
    "    assert(0.0 <= error_rate< 1.0)\n",
    "    if len(token) < 3:\n",
    "        return token\n",
    "    rand = np.random.rand()\n",
    "    prob = error_rate/4.0\n",
    "    if rand < prob:\n",
    "        # Replace a character with a random character.\n",
    "        random_char_index = np.random.randint(len(token))\n",
    "        token = token[:random_char_index] + np.random.choice(CHARS) \\\n",
    "                + token[random_char_index + 1:]\n",
    "    elif prob < rand < prob * 2:\n",
    "        # Delete a character.\n",
    "        random_char_index = np.random.randint(len(token))\n",
    "        token = token[:random_char_index] + token[random_char_index + 1:]\n",
    "    elif prob * 2 < rand < prob * 3:\n",
    "        # Add a random character.\n",
    "        random_char_index = np.random.randint(len(token))\n",
    "        token = token[:random_char_index] + np.random.choice(CHARS) \\\n",
    "                + token[random_char_index:]\n",
    "    elif prob * 3 < rand < prob * 4:\n",
    "        # Transpose 2 characters.\n",
    "        random_char_index = np.random.randint(len(token) - 1)\n",
    "        token = token[:random_char_index]  + token[random_char_index + 1] \\\n",
    "                + token[random_char_index] + token[random_char_index + 2:]\n",
    "    else:\n",
    "        # No spelling errors.\n",
    "        pass\n",
    "    return token\n",
    "\n",
    "\n",
    "def transform(tokens, maxlen, error_rate=0.3, shuffle=True):\n",
    "    \"\"\"Transform tokens into model inputs and targets.\n",
    "    All inputs and targets are padded to maxlen with EOS character.\n",
    "    \"\"\"\n",
    "    if shuffle:\n",
    "        print('Shuffling data.')\n",
    "        np.random.shuffle(tokens)\n",
    "    encoder_tokens = []\n",
    "    decoder_tokens = []\n",
    "    target_tokens = []\n",
    "    for token in tokens:\n",
    "        encoder = add_speling_erors(token, error_rate=error_rate)\n",
    "        encoder += EOS * (maxlen - len(encoder)) # Padded to maxlen.\n",
    "        encoder_tokens.append(encoder)\n",
    "    \n",
    "        decoder = SOS + token\n",
    "        decoder += EOS * (maxlen - len(decoder))\n",
    "        decoder_tokens.append(decoder)\n",
    "    \n",
    "        target = decoder[1:]\n",
    "        target += EOS * (maxlen - len(target))\n",
    "        target_tokens.append(target)\n",
    "        \n",
    "        assert(len(encoder) == len(decoder) == len(target))\n",
    "    return encoder_tokens, decoder_tokens, target_tokens\n",
    "\n",
    "\n",
    "def batch(tokens, maxlen, ctable,batch_size=128, reverse=False):\n",
    "    def generate(tokens, reverse):\n",
    "        while(True): # This flag yields an infinite generator.\n",
    "            for token in tokens:\n",
    "                if reverse:\n",
    "                    token = token[::-1]\n",
    "                yield token\n",
    "    \n",
    "    token_iterator = generate(tokens, reverse)\n",
    "    data_batch = np.zeros((batch_size, maxlen, ctable.size),\n",
    "                          dtype=np.float32)\n",
    "    while(True):\n",
    "        for i in range(batch_size):\n",
    "            token = next(token_iterator)\n",
    "            data_batch[i] = ctable.encode(token, maxlen)\n",
    "        yield data_batch\n",
    "\n",
    "\n",
    "def datagen(encoder_iter, decoder_iter, target_iter):\n",
    "    \"\"\"Utility function to load data into required model format.\"\"\"\n",
    "    inputs = zip(encoder_iter, decoder_iter)\n",
    "    while(True):\n",
    "        encoder_input, decoder_input = next(inputs)\n",
    "        target = next(target_iter)\n",
    "        yield ([encoder_input, decoder_input], target)\n",
    "\n",
    "\n",
    "def decode_sequences(inputs, targets, input_ctable, target_ctable,\n",
    "                     maxlen, reverse, encoder_model, decoder_model,\n",
    "                     nb_examples, sample_mode='argmax', random=True):\n",
    "    input_tokens = []\n",
    "    target_tokens = []\n",
    "    \n",
    "    if random:\n",
    "        indices = np.random.randint(0, len(inputs), nb_examples)\n",
    "    else:\n",
    "        indices = range(nb_examples)\n",
    "        \n",
    "    for index in indices:\n",
    "        input_tokens.append(inputs[index])\n",
    "        target_tokens.append(targets[index])\n",
    "    input_sequences = batch(input_tokens, maxlen, input_ctable,\n",
    "                            nb_examples, reverse)\n",
    "    input_sequences = next(input_sequences)   \n",
    "    states_value = encoder_model.predict(input_sequences)\n",
    "    target_sequences = np.zeros((nb_examples, 1, target_ctable.size))\n",
    "    target_sequences[:, 0, target_ctable.char2index[SOS]] = 1.0\n",
    "    decoded_tokens = [''] * nb_examples\n",
    "    for _ in range(maxlen):\n",
    "        char_probs, h, c = decoder_model.predict(\n",
    "            [target_sequences] + states_value)\n",
    "        target_sequences = np.zeros((nb_examples, 1, target_ctable.size))\n",
    "        sampled_chars = []\n",
    "        for i in range(nb_examples):\n",
    "            if sample_mode == 'argmax':\n",
    "                next_index, next_char = target_ctable.decode(\n",
    "                    char_probs[i], calc_argmax=True)\n",
    "            elif sample_mode == 'multinomial':\n",
    "                next_index, next_char = target_ctable.sample_multinomial(\n",
    "                    char_probs[i], temperature=0.5)\n",
    "            else:\n",
    "                raise Exception(\n",
    "                    \"`sample_mode` accepts `argmax` or `multinomial`.\")\n",
    "            decoded_tokens[i] += next_char\n",
    "            sampled_chars.append(next_char)\n",
    "            target_sequences[i, 0, next_index] = 1.0\n",
    "\n",
    "        stop_char = set(sampled_chars)\n",
    "        if len(stop_char) == 1 and stop_char.pop() == EOS:\n",
    "            break\n",
    "        states_value = [h, c]\n",
    "\n",
    "    input_tokens   = [re.sub('[%s]' % EOS, '', token)\n",
    "                      for token in input_tokens]\n",
    "    target_tokens  = [re.sub('[%s]' % EOS, '', token)\n",
    "                      for token in target_tokens]\n",
    "    decoded_tokens = [re.sub('[%s]' % EOS, '', token)\n",
    "                      for token in decoded_tokens]\n",
    "    return input_tokens, target_tokens, decoded_tokens\n",
    "\n",
    "\n",
    "def restore_model(path_to_full_model, hidden_size):\n",
    "    \"\"\"Restore model to construct the encoder and decoder.\"\"\"\n",
    "    model = load_model(path_to_full_model, custom_objects={\n",
    "        'truncated_acc': truncated_acc, 'truncated_loss': truncated_loss})\n",
    "    \n",
    "    encoder_inputs = model.input[0] # encoder_data\n",
    "    encoder_lstm1 = model.get_layer('encoder_lstm_1')\n",
    "    encoder_lstm2 = model.get_layer('encoder_lstm_2')\n",
    "    \n",
    "    encoder_outputs = encoder_lstm1(encoder_inputs)\n",
    "    _, state_h, state_c = encoder_lstm2(encoder_outputs)\n",
    "    encoder_states = [state_h, state_c]\n",
    "    encoder_model = Model(inputs=encoder_inputs, outputs=encoder_states)\n",
    "\n",
    "    decoder_inputs = model.input[1] # decoder_data\n",
    "    decoder_state_input_h = Input(shape=(hidden_size,))\n",
    "    decoder_state_input_c = Input(shape=(hidden_size,))\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    decoder_lstm = model.get_layer('decoder_lstm')\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "        decoder_inputs, initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_softmax = model.get_layer('decoder_softmax')\n",
    "    decoder_outputs = decoder_softmax(decoder_outputs)\n",
    "    decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs,\n",
    "                          outputs=[decoder_outputs] + decoder_states)\n",
    "    return encoder_model, decoder_model\n",
    "\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Dropout\n",
    "from keras import optimizers, metrics, backend as K\n",
    "VAL_MAXLEN = 16\n",
    "def truncated_acc(y_true, y_pred):\n",
    "    y_true = y_true[:, :VAL_MAXLEN, :]\n",
    "    y_pred = y_pred[:, :VAL_MAXLEN, :]\n",
    "    \n",
    "    acc = metrics.categorical_accuracy(y_true, y_pred)\n",
    "    return K.mean(acc, axis=-1)\n",
    "\n",
    "def truncated_loss(y_true, y_pred):\n",
    "    y_true = y_true[:, :VAL_MAXLEN, :]\n",
    "    y_pred = y_pred[:, :VAL_MAXLEN, :]\n",
    "    \n",
    "    loss = K.categorical_crossentropy(\n",
    "        target=y_true, output=y_pred, from_logits=False)\n",
    "    return K.mean(loss, axis=-1)\n",
    "\n",
    "def seq2seq(hidden_size, nb_input_chars, nb_target_chars):\n",
    "    encoder_inputs = Input(shape=(None, nb_input_chars),\n",
    "                           name='encoder_data')\n",
    "    encoder_lstm = LSTM(hidden_size, recurrent_dropout=0.2,\n",
    "                        return_sequences=True, return_state=False,\n",
    "                        name='encoder_lstm_1')\n",
    "    encoder_outputs = encoder_lstm(encoder_inputs)\n",
    "    \n",
    "    encoder_lstm = LSTM(hidden_size, recurrent_dropout=0.2,\n",
    "                        return_sequences=False, return_state=True,\n",
    "                        name='encoder_lstm_2')\n",
    "    encoder_outputs, state_h, state_c = encoder_lstm(encoder_outputs)\n",
    "    encoder_states = [state_h, state_c]\n",
    "    decoder_inputs = Input(shape=(None, nb_target_chars),\n",
    "                           name='decoder_data')\n",
    "    decoder_lstm = LSTM(hidden_size, dropout=0.2, return_sequences=True,\n",
    "                        return_state=True, name='decoder_lstm')\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                         initial_state=encoder_states)\n",
    "    decoder_softmax = Dense(nb_target_chars, activation='softmax',\n",
    "                            name='decoder_softmax')\n",
    "    decoder_outputs = decoder_softmax(decoder_outputs)\n",
    "\n",
    "    # The main model will turn `encoder_input_data` & `decoder_input_data`\n",
    "    # into `decoder_target_data`\n",
    "    model = Model(inputs=[encoder_inputs, decoder_inputs],\n",
    "                  outputs=decoder_outputs)\n",
    "    \n",
    "    adam = optimizers.Adam(lr=0.001, decay=0.0)\n",
    "    model.compile(optimizer=adam, loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy', truncated_acc, truncated_loss])\n",
    "    encoder_model = Model(inputs=encoder_inputs, outputs=encoder_states)\n",
    "    decoder_state_input_h = Input(shape=(hidden_size,))\n",
    "    decoder_state_input_c = Input(shape=(hidden_size,))\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "        decoder_inputs, initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_softmax(decoder_outputs)\n",
    "    decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs,\n",
    "                          outputs=[decoder_outputs] + decoder_states)\n",
    "\n",
    "    return model, encoder_model, decoder_model\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "error_rate = 0.6\n",
    "reverse = True\n",
    "model_path =r'C:\\Users\\ganes\\Music\\Python Scripts\\checkpoints\\seq2seq_epoch_100.h5'\n",
    "hidden_size = 512\n",
    "sample_mode = 'argmax'\n",
    "data_path =r'C:\\Users\\ganes\\Music\\data'\n",
    "books = ['nietzsche.txt', 'pride_and_prejudice.txt', 'shakespeare.txt', 'war_and_peace.txt']\n",
    "test_sentence = str(input(\"ENTER THE TEXT : \"))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    text  = read_text(data_path, books)\n",
    "    vocab = tokenize(text)\n",
    "    vocab = list(filter(None, set(vocab)))\n",
    "    # `maxlen` is the length of the longest word in the vocabulary\n",
    "    # plus two SOS and EOS characters.\n",
    "    maxlen = max([len(token) for token in vocab]) + 2\n",
    "    train_encoder, train_decoder, train_target = transform(\n",
    "        vocab, maxlen, error_rate=error_rate, shuffle=False)\n",
    "\n",
    "    tokens = tokenize(test_sentence)\n",
    "    tokens = list(filter(None, tokens))\n",
    "    nb_tokens = len(tokens)\n",
    "    misspelled_tokens, _, target_tokens = transform(\n",
    "        tokens, maxlen, error_rate=error_rate, shuffle=False)\n",
    "\n",
    "    input_chars = set(' '.join(train_encoder))\n",
    "    target_chars = set(' '.join(train_decoder))\n",
    "    input_ctable = CharacterTable(input_chars)\n",
    "    target_ctable = CharacterTable(target_chars)\n",
    "    \n",
    "    encoder_model, decoder_model = restore_model(model_path, hidden_size)\n",
    "    \n",
    "    input_tokens, target_tokens, decoded_tokens = decode_sequences(\n",
    "        misspelled_tokens, target_tokens, input_ctable, target_ctable,\n",
    "        maxlen, reverse, encoder_model, decoder_model, nb_tokens,\n",
    "        sample_mode=sample_mode, random=False)\n",
    "    \n",
    "    print('-')\n",
    "    print('Input sentence:  ', ' '.join([token for token in input_tokens]))\n",
    "    print('-')\n",
    "    print('Decoded sentence:', ' '.join([token for token in decoded_tokens]))\n",
    "    print('-')\n",
    "    print('Target sentence: ', ' '.join([token for token in target_tokens]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
